{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "145_PyTorch_Tricks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "T8fuVdeJ2ily",
        "fS8Qzs_S8Lo3",
        "bGFUQPW4EzyW"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sylar257/My-data-science-tool-kit/blob/master/145_PyTorch_Tricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_rSH7id15Fy",
        "colab_type": "text"
      },
      "source": [
        "# 145 PyTorch Tricks\n",
        "This is a series of useful PyTorch tricks inspired by **vainaijr** in his [YouTube channel](https://www.youtube.com/watch?v=nnHQT9JnY74&list=PLUY8w37x-QUUkawz-cBnjLpvaZWvPZh_s&index=2&t=29s).<br>\n",
        "This notebook is an implementation of all these techniques and is designed in a way to best demonstrate their usefulness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8fuVdeJ2ily",
        "colab_type": "text"
      },
      "source": [
        "## Trick #1\n",
        "Visualization model using `torchsummaryX`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQnXYH10AdtN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from Utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTuqtpoh5_mF",
        "colab_type": "text"
      },
      "source": [
        "Here we will build a Single-shot-detection model with just 20 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzAiSJmbAz4h",
        "colab_type": "code",
        "outputId": "23be6a00-28b3-44c1-e45f-9f738eeb8158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# Create SSD300 with pretrained weights in the base-architecture\n",
        "n_classes = 20\n",
        "model = SSD300(n_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/checkpoints/vgg16_bn-6c64b313.pth\n",
            "100%|██████████| 528M/528M [00:05<00:00, 107MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded base model with pre-trained weights\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbRH1ipJBrYR",
        "colab_type": "code",
        "outputId": "51470b25-223b-4fe8-d256-95eba43b1a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# install torchsummaryX\n",
        "!pip install torchsummaryX"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchsummaryX\n",
            "  Downloading https://files.pythonhosted.org/packages/36/23/87eeaaf70daa61aa21495ece0969c50c446b8fd42c4b8905af264b40fe7f/torchsummaryX-1.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (0.25.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.3.1+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchsummaryX) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryX) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->torchsummaryX) (1.12.0)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcUf7Bgp6dZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchsummaryX import summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B78vIOLG6vY9",
        "colab_type": "text"
      },
      "source": [
        "`summary(model, input)` takes our intentional model and a pseudo input **with the correct shape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na0Hgjdv6pLu",
        "colab_type": "code",
        "outputId": "1fa823e8-b2e6-486a-aef0-3b9632d8eb09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# pseudo input of batch size = 3, num_channel = 3, pixel: 300x300\n",
        "summary(model, torch.zeros((3,3,300,300)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================================================================\n",
            "                                         Kernel Shape        Output Shape  \\\n",
            "Layer                                                                       \n",
            "0_base.Conv2d_conv1_1                   [3, 64, 3, 3]   [3, 64, 300, 300]   \n",
            "1_base.BatchNorm2d_bn_1_1                        [64]   [3, 64, 300, 300]   \n",
            "2_base.Conv2d_conv1_2                  [64, 64, 3, 3]   [3, 64, 300, 300]   \n",
            "3_base.BatchNorm2d_bn_1_2                        [64]   [3, 64, 300, 300]   \n",
            "4_base.MaxPool2d_pool1                              -   [3, 64, 150, 150]   \n",
            "5_base.Conv2d_conv2_1                 [64, 128, 3, 3]  [3, 128, 150, 150]   \n",
            "6_base.BatchNorm2d_bn_2_1                       [128]  [3, 128, 150, 150]   \n",
            "7_base.Conv2d_conv2_2                [128, 128, 3, 3]  [3, 128, 150, 150]   \n",
            "8_base.BatchNorm2d_bn_2_2                       [128]  [3, 128, 150, 150]   \n",
            "9_base.MaxPool2d_pool2                              -    [3, 128, 75, 75]   \n",
            "10_base.Conv2d_conv3_1               [128, 256, 3, 3]    [3, 256, 75, 75]   \n",
            "11_base.BatchNorm2d_bn_3_1                      [256]    [3, 256, 75, 75]   \n",
            "12_base.Conv2d_conv3_2               [256, 256, 3, 3]    [3, 256, 75, 75]   \n",
            "13_base.BatchNorm2d_bn_3_2                      [256]    [3, 256, 75, 75]   \n",
            "14_base.Conv2d_conv3_3               [256, 256, 3, 3]    [3, 256, 75, 75]   \n",
            "15_base.BatchNorm2d_bn_3_3                      [256]    [3, 256, 75, 75]   \n",
            "16_base.MaxPool2d_pool3                             -    [3, 256, 38, 38]   \n",
            "17_base.Conv2d_conv4_1               [256, 512, 3, 3]    [3, 512, 38, 38]   \n",
            "18_base.BatchNorm2d_bn_4_1                      [512]    [3, 512, 38, 38]   \n",
            "19_base.Conv2d_conv4_2               [512, 512, 3, 3]    [3, 512, 38, 38]   \n",
            "20_base.BatchNorm2d_bn_4_2                      [512]    [3, 512, 38, 38]   \n",
            "21_base.Conv2d_conv4_3               [512, 512, 3, 3]    [3, 512, 38, 38]   \n",
            "22_base.BatchNorm2d_bn_4_3                      [512]    [3, 512, 38, 38]   \n",
            "23_base.MaxPool2d_pool4                             -    [3, 512, 19, 19]   \n",
            "24_base.Conv2d_conv5_1               [512, 512, 3, 3]    [3, 512, 19, 19]   \n",
            "25_base.BatchNorm2d_bn_5_1                      [512]    [3, 512, 19, 19]   \n",
            "26_base.Conv2d_conv5_2               [512, 512, 3, 3]    [3, 512, 19, 19]   \n",
            "27_base.BatchNorm2d_bn_5_2                      [512]    [3, 512, 19, 19]   \n",
            "28_base.Conv2d_conv5_3               [512, 512, 3, 3]    [3, 512, 19, 19]   \n",
            "29_base.BatchNorm2d_bn_5_3                      [512]    [3, 512, 19, 19]   \n",
            "30_base.MaxPool2d_pool5                             -    [3, 512, 19, 19]   \n",
            "31_base.Conv2d_conv6                [512, 1024, 3, 3]   [3, 1024, 19, 19]   \n",
            "32_base.Conv2d_conv7               [1024, 1024, 1, 1]   [3, 1024, 19, 19]   \n",
            "33_aux_convs.Conv2d_conv8_1         [1024, 256, 1, 1]    [3, 256, 19, 19]   \n",
            "34_aux_convs.Conv2d_conv8_2          [256, 512, 3, 3]    [3, 512, 10, 10]   \n",
            "35_aux_convs.Conv2d_conv9_1          [512, 128, 1, 1]    [3, 128, 10, 10]   \n",
            "36_aux_convs.Conv2d_conv9_2          [128, 256, 3, 3]      [3, 256, 5, 5]   \n",
            "37_aux_convs.Conv2d_conv10_1         [256, 128, 1, 1]      [3, 128, 5, 5]   \n",
            "38_aux_convs.Conv2d_conv10_2         [128, 256, 3, 3]      [3, 256, 3, 3]   \n",
            "39_aux_convs.Conv2d_conv11_1         [256, 128, 1, 1]      [3, 128, 3, 3]   \n",
            "40_aux_convs.Conv2d_conv11_2         [128, 256, 3, 3]      [3, 256, 1, 1]   \n",
            "41_pred_convs.Conv2d_loc_conv4_3      [512, 16, 3, 3]     [3, 16, 38, 38]   \n",
            "42_pred_convs.Conv2d_loc_conv7       [1024, 24, 3, 3]     [3, 24, 19, 19]   \n",
            "43_pred_convs.Conv2d_loc_conv8_2      [512, 24, 3, 3]     [3, 24, 10, 10]   \n",
            "44_pred_convs.Conv2d_loc_conv9_2      [256, 24, 3, 3]       [3, 24, 5, 5]   \n",
            "45_pred_convs.Conv2d_loc_conv10_2     [256, 16, 3, 3]       [3, 16, 3, 3]   \n",
            "46_pred_convs.Conv2d_loc_conv11_2     [256, 16, 3, 3]       [3, 16, 1, 1]   \n",
            "47_pred_convs.Conv2d_cl_conv4_3       [512, 80, 3, 3]     [3, 80, 38, 38]   \n",
            "48_pred_convs.Conv2d_cl_conv7       [1024, 120, 3, 3]    [3, 120, 19, 19]   \n",
            "49_pred_convs.Conv2d_cl_conv8_2      [512, 120, 3, 3]    [3, 120, 10, 10]   \n",
            "50_pred_convs.Conv2d_cl_conv9_2      [256, 120, 3, 3]      [3, 120, 5, 5]   \n",
            "51_pred_convs.Conv2d_cl_conv10_2      [256, 80, 3, 3]       [3, 80, 3, 3]   \n",
            "52_pred_convs.Conv2d_cl_conv11_2      [256, 80, 3, 3]       [3, 80, 1, 1]   \n",
            "\n",
            "                                      Params     Mult-Adds  \n",
            "Layer                                                       \n",
            "0_base.Conv2d_conv1_1                 1.792k       155.52M  \n",
            "1_base.BatchNorm2d_bn_1_1              128.0          64.0  \n",
            "2_base.Conv2d_conv1_2                36.928k      3.31776G  \n",
            "3_base.BatchNorm2d_bn_1_2              128.0          64.0  \n",
            "4_base.MaxPool2d_pool1                     -             -  \n",
            "5_base.Conv2d_conv2_1                73.856k      1.65888G  \n",
            "6_base.BatchNorm2d_bn_2_1              256.0         128.0  \n",
            "7_base.Conv2d_conv2_2               147.584k      3.31776G  \n",
            "8_base.BatchNorm2d_bn_2_2              256.0         128.0  \n",
            "9_base.MaxPool2d_pool2                     -             -  \n",
            "10_base.Conv2d_conv3_1              295.168k      1.65888G  \n",
            "11_base.BatchNorm2d_bn_3_1             512.0         256.0  \n",
            "12_base.Conv2d_conv3_2               590.08k      3.31776G  \n",
            "13_base.BatchNorm2d_bn_3_2             512.0         256.0  \n",
            "14_base.Conv2d_conv3_3               590.08k      3.31776G  \n",
            "15_base.BatchNorm2d_bn_3_3             512.0         256.0  \n",
            "16_base.MaxPool2d_pool3                    -             -  \n",
            "17_base.Conv2d_conv4_1              1.18016M  1.703411712G  \n",
            "18_base.BatchNorm2d_bn_4_1            1.024k         512.0  \n",
            "19_base.Conv2d_conv4_2             2.359808M  3.406823424G  \n",
            "20_base.BatchNorm2d_bn_4_2            1.024k         512.0  \n",
            "21_base.Conv2d_conv4_3             2.359808M  3.406823424G  \n",
            "22_base.BatchNorm2d_bn_4_3            1.024k         512.0  \n",
            "23_base.MaxPool2d_pool4                    -             -  \n",
            "24_base.Conv2d_conv5_1             2.359808M   851.705856M  \n",
            "25_base.BatchNorm2d_bn_5_1            1.024k         512.0  \n",
            "26_base.Conv2d_conv5_2             2.359808M   851.705856M  \n",
            "27_base.BatchNorm2d_bn_5_2            1.024k         512.0  \n",
            "28_base.Conv2d_conv5_3             2.359808M   851.705856M  \n",
            "29_base.BatchNorm2d_bn_5_3            1.024k         512.0  \n",
            "30_base.MaxPool2d_pool5                    -             -  \n",
            "31_base.Conv2d_conv6               4.719616M  1.703411712G  \n",
            "32_base.Conv2d_conv7                 1.0496M   378.535936M  \n",
            "33_aux_convs.Conv2d_conv8_1           262.4k    94.633984M  \n",
            "34_aux_convs.Conv2d_conv8_2         1.18016M     117.9648M  \n",
            "35_aux_convs.Conv2d_conv9_1          65.664k       6.5536M  \n",
            "36_aux_convs.Conv2d_conv9_2         295.168k       7.3728M  \n",
            "37_aux_convs.Conv2d_conv10_1         32.896k        819.2k  \n",
            "38_aux_convs.Conv2d_conv10_2        295.168k     2.654208M  \n",
            "39_aux_convs.Conv2d_conv11_1         32.896k      294.912k  \n",
            "40_aux_convs.Conv2d_conv11_2        295.168k      294.912k  \n",
            "41_pred_convs.Conv2d_loc_conv4_3     73.744k   106.463232M  \n",
            "42_pred_convs.Conv2d_loc_conv7      221.208k    79.847424M  \n",
            "43_pred_convs.Conv2d_loc_conv8_2    110.616k      11.0592M  \n",
            "44_pred_convs.Conv2d_loc_conv9_2      55.32k       1.3824M  \n",
            "45_pred_convs.Conv2d_loc_conv10_2     36.88k      331.776k  \n",
            "46_pred_convs.Conv2d_loc_conv11_2     36.88k       36.864k  \n",
            "47_pred_convs.Conv2d_cl_conv4_3      368.72k    532.31616M  \n",
            "48_pred_convs.Conv2d_cl_conv7       1.10604M    399.23712M  \n",
            "49_pred_convs.Conv2d_cl_conv8_2      553.08k       55.296M  \n",
            "50_pred_convs.Conv2d_cl_conv9_2       276.6k        6.912M  \n",
            "51_pred_convs.Conv2d_cl_conv10_2      184.4k      1.65888M  \n",
            "52_pred_convs.Conv2d_cl_conv11_2      184.4k       184.32k  \n",
            "--------------------------------------------------------------------------------------------------\n",
            "                             Totals\n",
            "Total params              26.15976M\n",
            "Trainable params          26.15976M\n",
            "Non-trainable params            0.0\n",
            "Mult-Adds             31.323761792G\n",
            "==================================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_base.Conv2d_conv1_1</th>\n",
              "      <td>[3, 64, 3, 3]</td>\n",
              "      <td>[3, 64, 300, 300]</td>\n",
              "      <td>1792.0</td>\n",
              "      <td>1.555200e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_base.BatchNorm2d_bn_1_1</th>\n",
              "      <td>[64]</td>\n",
              "      <td>[3, 64, 300, 300]</td>\n",
              "      <td>128.0</td>\n",
              "      <td>6.400000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_base.Conv2d_conv1_2</th>\n",
              "      <td>[64, 64, 3, 3]</td>\n",
              "      <td>[3, 64, 300, 300]</td>\n",
              "      <td>36928.0</td>\n",
              "      <td>3.317760e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_base.BatchNorm2d_bn_1_2</th>\n",
              "      <td>[64]</td>\n",
              "      <td>[3, 64, 300, 300]</td>\n",
              "      <td>128.0</td>\n",
              "      <td>6.400000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_base.MaxPool2d_pool1</th>\n",
              "      <td>-</td>\n",
              "      <td>[3, 64, 150, 150]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_base.Conv2d_conv2_1</th>\n",
              "      <td>[64, 128, 3, 3]</td>\n",
              "      <td>[3, 128, 150, 150]</td>\n",
              "      <td>73856.0</td>\n",
              "      <td>1.658880e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_base.BatchNorm2d_bn_2_1</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[3, 128, 150, 150]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>1.280000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_base.Conv2d_conv2_2</th>\n",
              "      <td>[128, 128, 3, 3]</td>\n",
              "      <td>[3, 128, 150, 150]</td>\n",
              "      <td>147584.0</td>\n",
              "      <td>3.317760e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_base.BatchNorm2d_bn_2_2</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[3, 128, 150, 150]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>1.280000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_base.MaxPool2d_pool2</th>\n",
              "      <td>-</td>\n",
              "      <td>[3, 128, 75, 75]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_base.Conv2d_conv3_1</th>\n",
              "      <td>[128, 256, 3, 3]</td>\n",
              "      <td>[3, 256, 75, 75]</td>\n",
              "      <td>295168.0</td>\n",
              "      <td>1.658880e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_base.BatchNorm2d_bn_3_1</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[3, 256, 75, 75]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>2.560000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_base.Conv2d_conv3_2</th>\n",
              "      <td>[256, 256, 3, 3]</td>\n",
              "      <td>[3, 256, 75, 75]</td>\n",
              "      <td>590080.0</td>\n",
              "      <td>3.317760e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_base.BatchNorm2d_bn_3_2</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[3, 256, 75, 75]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>2.560000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_base.Conv2d_conv3_3</th>\n",
              "      <td>[256, 256, 3, 3]</td>\n",
              "      <td>[3, 256, 75, 75]</td>\n",
              "      <td>590080.0</td>\n",
              "      <td>3.317760e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_base.BatchNorm2d_bn_3_3</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[3, 256, 75, 75]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>2.560000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_base.MaxPool2d_pool3</th>\n",
              "      <td>-</td>\n",
              "      <td>[3, 256, 38, 38]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_base.Conv2d_conv4_1</th>\n",
              "      <td>[256, 512, 3, 3]</td>\n",
              "      <td>[3, 512, 38, 38]</td>\n",
              "      <td>1180160.0</td>\n",
              "      <td>1.703412e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_base.BatchNorm2d_bn_4_1</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[3, 512, 38, 38]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_base.Conv2d_conv4_2</th>\n",
              "      <td>[512, 512, 3, 3]</td>\n",
              "      <td>[3, 512, 38, 38]</td>\n",
              "      <td>2359808.0</td>\n",
              "      <td>3.406823e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_base.BatchNorm2d_bn_4_2</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[3, 512, 38, 38]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_base.Conv2d_conv4_3</th>\n",
              "      <td>[512, 512, 3, 3]</td>\n",
              "      <td>[3, 512, 38, 38]</td>\n",
              "      <td>2359808.0</td>\n",
              "      <td>3.406823e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_base.BatchNorm2d_bn_4_3</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[3, 512, 38, 38]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_base.MaxPool2d_pool4</th>\n",
              "      <td>-</td>\n",
              "      <td>[3, 512, 19, 19]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_base.Conv2d_conv5_1</th>\n",
              "      <td>[512, 512, 3, 3]</td>\n",
              "      <td>[3, 512, 19, 19]</td>\n",
              "      <td>2359808.0</td>\n",
              "      <td>8.517059e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_base.BatchNorm2d_bn_5_1</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[3, 512, 19, 19]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_base.Conv2d_conv5_2</th>\n",
              "      <td>[512, 512, 3, 3]</td>\n",
              "      <td>[3, 512, 19, 19]</td>\n",
              "      <td>2359808.0</td>\n",
              "      <td>8.517059e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_base.BatchNorm2d_bn_5_2</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[3, 512, 19, 19]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_base.Conv2d_conv5_3</th>\n",
              "      <td>[512, 512, 3, 3]</td>\n",
              "      <td>[3, 512, 19, 19]</td>\n",
              "      <td>2359808.0</td>\n",
              "      <td>8.517059e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_base.BatchNorm2d_bn_5_3</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[3, 512, 19, 19]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>5.120000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30_base.MaxPool2d_pool5</th>\n",
              "      <td>-</td>\n",
              "      <td>[3, 512, 19, 19]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31_base.Conv2d_conv6</th>\n",
              "      <td>[512, 1024, 3, 3]</td>\n",
              "      <td>[3, 1024, 19, 19]</td>\n",
              "      <td>4719616.0</td>\n",
              "      <td>1.703412e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32_base.Conv2d_conv7</th>\n",
              "      <td>[1024, 1024, 1, 1]</td>\n",
              "      <td>[3, 1024, 19, 19]</td>\n",
              "      <td>1049600.0</td>\n",
              "      <td>3.785359e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33_aux_convs.Conv2d_conv8_1</th>\n",
              "      <td>[1024, 256, 1, 1]</td>\n",
              "      <td>[3, 256, 19, 19]</td>\n",
              "      <td>262400.0</td>\n",
              "      <td>9.463398e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34_aux_convs.Conv2d_conv8_2</th>\n",
              "      <td>[256, 512, 3, 3]</td>\n",
              "      <td>[3, 512, 10, 10]</td>\n",
              "      <td>1180160.0</td>\n",
              "      <td>1.179648e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35_aux_convs.Conv2d_conv9_1</th>\n",
              "      <td>[512, 128, 1, 1]</td>\n",
              "      <td>[3, 128, 10, 10]</td>\n",
              "      <td>65664.0</td>\n",
              "      <td>6.553600e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36_aux_convs.Conv2d_conv9_2</th>\n",
              "      <td>[128, 256, 3, 3]</td>\n",
              "      <td>[3, 256, 5, 5]</td>\n",
              "      <td>295168.0</td>\n",
              "      <td>7.372800e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37_aux_convs.Conv2d_conv10_1</th>\n",
              "      <td>[256, 128, 1, 1]</td>\n",
              "      <td>[3, 128, 5, 5]</td>\n",
              "      <td>32896.0</td>\n",
              "      <td>8.192000e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38_aux_convs.Conv2d_conv10_2</th>\n",
              "      <td>[128, 256, 3, 3]</td>\n",
              "      <td>[3, 256, 3, 3]</td>\n",
              "      <td>295168.0</td>\n",
              "      <td>2.654208e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39_aux_convs.Conv2d_conv11_1</th>\n",
              "      <td>[256, 128, 1, 1]</td>\n",
              "      <td>[3, 128, 3, 3]</td>\n",
              "      <td>32896.0</td>\n",
              "      <td>2.949120e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40_aux_convs.Conv2d_conv11_2</th>\n",
              "      <td>[128, 256, 3, 3]</td>\n",
              "      <td>[3, 256, 1, 1]</td>\n",
              "      <td>295168.0</td>\n",
              "      <td>2.949120e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41_pred_convs.Conv2d_loc_conv4_3</th>\n",
              "      <td>[512, 16, 3, 3]</td>\n",
              "      <td>[3, 16, 38, 38]</td>\n",
              "      <td>73744.0</td>\n",
              "      <td>1.064632e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42_pred_convs.Conv2d_loc_conv7</th>\n",
              "      <td>[1024, 24, 3, 3]</td>\n",
              "      <td>[3, 24, 19, 19]</td>\n",
              "      <td>221208.0</td>\n",
              "      <td>7.984742e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43_pred_convs.Conv2d_loc_conv8_2</th>\n",
              "      <td>[512, 24, 3, 3]</td>\n",
              "      <td>[3, 24, 10, 10]</td>\n",
              "      <td>110616.0</td>\n",
              "      <td>1.105920e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44_pred_convs.Conv2d_loc_conv9_2</th>\n",
              "      <td>[256, 24, 3, 3]</td>\n",
              "      <td>[3, 24, 5, 5]</td>\n",
              "      <td>55320.0</td>\n",
              "      <td>1.382400e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45_pred_convs.Conv2d_loc_conv10_2</th>\n",
              "      <td>[256, 16, 3, 3]</td>\n",
              "      <td>[3, 16, 3, 3]</td>\n",
              "      <td>36880.0</td>\n",
              "      <td>3.317760e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46_pred_convs.Conv2d_loc_conv11_2</th>\n",
              "      <td>[256, 16, 3, 3]</td>\n",
              "      <td>[3, 16, 1, 1]</td>\n",
              "      <td>36880.0</td>\n",
              "      <td>3.686400e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47_pred_convs.Conv2d_cl_conv4_3</th>\n",
              "      <td>[512, 80, 3, 3]</td>\n",
              "      <td>[3, 80, 38, 38]</td>\n",
              "      <td>368720.0</td>\n",
              "      <td>5.323162e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48_pred_convs.Conv2d_cl_conv7</th>\n",
              "      <td>[1024, 120, 3, 3]</td>\n",
              "      <td>[3, 120, 19, 19]</td>\n",
              "      <td>1106040.0</td>\n",
              "      <td>3.992371e+08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49_pred_convs.Conv2d_cl_conv8_2</th>\n",
              "      <td>[512, 120, 3, 3]</td>\n",
              "      <td>[3, 120, 10, 10]</td>\n",
              "      <td>553080.0</td>\n",
              "      <td>5.529600e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50_pred_convs.Conv2d_cl_conv9_2</th>\n",
              "      <td>[256, 120, 3, 3]</td>\n",
              "      <td>[3, 120, 5, 5]</td>\n",
              "      <td>276600.0</td>\n",
              "      <td>6.912000e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51_pred_convs.Conv2d_cl_conv10_2</th>\n",
              "      <td>[256, 80, 3, 3]</td>\n",
              "      <td>[3, 80, 3, 3]</td>\n",
              "      <td>184400.0</td>\n",
              "      <td>1.658880e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52_pred_convs.Conv2d_cl_conv11_2</th>\n",
              "      <td>[256, 80, 3, 3]</td>\n",
              "      <td>[3, 80, 1, 1]</td>\n",
              "      <td>184400.0</td>\n",
              "      <td>1.843200e+05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Kernel Shape  ...     Mult-Adds\n",
              "Layer                                                  ...              \n",
              "0_base.Conv2d_conv1_1                   [3, 64, 3, 3]  ...  1.555200e+08\n",
              "1_base.BatchNorm2d_bn_1_1                        [64]  ...  6.400000e+01\n",
              "2_base.Conv2d_conv1_2                  [64, 64, 3, 3]  ...  3.317760e+09\n",
              "3_base.BatchNorm2d_bn_1_2                        [64]  ...  6.400000e+01\n",
              "4_base.MaxPool2d_pool1                              -  ...           NaN\n",
              "5_base.Conv2d_conv2_1                 [64, 128, 3, 3]  ...  1.658880e+09\n",
              "6_base.BatchNorm2d_bn_2_1                       [128]  ...  1.280000e+02\n",
              "7_base.Conv2d_conv2_2                [128, 128, 3, 3]  ...  3.317760e+09\n",
              "8_base.BatchNorm2d_bn_2_2                       [128]  ...  1.280000e+02\n",
              "9_base.MaxPool2d_pool2                              -  ...           NaN\n",
              "10_base.Conv2d_conv3_1               [128, 256, 3, 3]  ...  1.658880e+09\n",
              "11_base.BatchNorm2d_bn_3_1                      [256]  ...  2.560000e+02\n",
              "12_base.Conv2d_conv3_2               [256, 256, 3, 3]  ...  3.317760e+09\n",
              "13_base.BatchNorm2d_bn_3_2                      [256]  ...  2.560000e+02\n",
              "14_base.Conv2d_conv3_3               [256, 256, 3, 3]  ...  3.317760e+09\n",
              "15_base.BatchNorm2d_bn_3_3                      [256]  ...  2.560000e+02\n",
              "16_base.MaxPool2d_pool3                             -  ...           NaN\n",
              "17_base.Conv2d_conv4_1               [256, 512, 3, 3]  ...  1.703412e+09\n",
              "18_base.BatchNorm2d_bn_4_1                      [512]  ...  5.120000e+02\n",
              "19_base.Conv2d_conv4_2               [512, 512, 3, 3]  ...  3.406823e+09\n",
              "20_base.BatchNorm2d_bn_4_2                      [512]  ...  5.120000e+02\n",
              "21_base.Conv2d_conv4_3               [512, 512, 3, 3]  ...  3.406823e+09\n",
              "22_base.BatchNorm2d_bn_4_3                      [512]  ...  5.120000e+02\n",
              "23_base.MaxPool2d_pool4                             -  ...           NaN\n",
              "24_base.Conv2d_conv5_1               [512, 512, 3, 3]  ...  8.517059e+08\n",
              "25_base.BatchNorm2d_bn_5_1                      [512]  ...  5.120000e+02\n",
              "26_base.Conv2d_conv5_2               [512, 512, 3, 3]  ...  8.517059e+08\n",
              "27_base.BatchNorm2d_bn_5_2                      [512]  ...  5.120000e+02\n",
              "28_base.Conv2d_conv5_3               [512, 512, 3, 3]  ...  8.517059e+08\n",
              "29_base.BatchNorm2d_bn_5_3                      [512]  ...  5.120000e+02\n",
              "30_base.MaxPool2d_pool5                             -  ...           NaN\n",
              "31_base.Conv2d_conv6                [512, 1024, 3, 3]  ...  1.703412e+09\n",
              "32_base.Conv2d_conv7               [1024, 1024, 1, 1]  ...  3.785359e+08\n",
              "33_aux_convs.Conv2d_conv8_1         [1024, 256, 1, 1]  ...  9.463398e+07\n",
              "34_aux_convs.Conv2d_conv8_2          [256, 512, 3, 3]  ...  1.179648e+08\n",
              "35_aux_convs.Conv2d_conv9_1          [512, 128, 1, 1]  ...  6.553600e+06\n",
              "36_aux_convs.Conv2d_conv9_2          [128, 256, 3, 3]  ...  7.372800e+06\n",
              "37_aux_convs.Conv2d_conv10_1         [256, 128, 1, 1]  ...  8.192000e+05\n",
              "38_aux_convs.Conv2d_conv10_2         [128, 256, 3, 3]  ...  2.654208e+06\n",
              "39_aux_convs.Conv2d_conv11_1         [256, 128, 1, 1]  ...  2.949120e+05\n",
              "40_aux_convs.Conv2d_conv11_2         [128, 256, 3, 3]  ...  2.949120e+05\n",
              "41_pred_convs.Conv2d_loc_conv4_3      [512, 16, 3, 3]  ...  1.064632e+08\n",
              "42_pred_convs.Conv2d_loc_conv7       [1024, 24, 3, 3]  ...  7.984742e+07\n",
              "43_pred_convs.Conv2d_loc_conv8_2      [512, 24, 3, 3]  ...  1.105920e+07\n",
              "44_pred_convs.Conv2d_loc_conv9_2      [256, 24, 3, 3]  ...  1.382400e+06\n",
              "45_pred_convs.Conv2d_loc_conv10_2     [256, 16, 3, 3]  ...  3.317760e+05\n",
              "46_pred_convs.Conv2d_loc_conv11_2     [256, 16, 3, 3]  ...  3.686400e+04\n",
              "47_pred_convs.Conv2d_cl_conv4_3       [512, 80, 3, 3]  ...  5.323162e+08\n",
              "48_pred_convs.Conv2d_cl_conv7       [1024, 120, 3, 3]  ...  3.992371e+08\n",
              "49_pred_convs.Conv2d_cl_conv8_2      [512, 120, 3, 3]  ...  5.529600e+07\n",
              "50_pred_convs.Conv2d_cl_conv9_2      [256, 120, 3, 3]  ...  6.912000e+06\n",
              "51_pred_convs.Conv2d_cl_conv10_2      [256, 80, 3, 3]  ...  1.658880e+06\n",
              "52_pred_convs.Conv2d_cl_conv11_2      [256, 80, 3, 3]  ...  1.843200e+05\n",
              "\n",
              "[53 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHgGy0J07PYI",
        "colab_type": "text"
      },
      "source": [
        "Final Note: Normally, if we use architectures directly from `TorchVision` or `Keras` we would have nice model summary just like this.<br>\n",
        "This libarary is particular useful when we want to inspect user people's model or a verions that we have modified besed on commonly used models like the example above.<br>\n",
        "In addition, we have a nice visualization of **num of parameters** & **output demension** for each layer which is kind of nice for debugging your own model or simply for reference.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS8Qzs_S8Lo3",
        "colab_type": "text"
      },
      "source": [
        "# Trick #2\n",
        "PyTorch Hooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWIJf1TcEv03",
        "colab_type": "text"
      },
      "source": [
        "PyTorch hook is a tool that we can *register* to any **tensor** or **nn.Module** during our computation so that we can monitor what is going on with our `forward` and `backward` loops.<bR>\n",
        "The `forward` is not refered to `nn.Module.forward` bu the `torch.Autograd.Function` object that is the `grad_fn` of a **tensor**.<br>\n",
        "Notice, that a `nn.Module` like `nn.Linear` can have multiple `forward` invocations. It's output is created by two operations, $Y = W*X+B$, *addition* and *multiplication* and thus there will be two `forward` calls. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGFUQPW4EzyW",
        "colab_type": "text"
      },
      "source": [
        "## Hook types\n",
        "1. The Forward Hook\n",
        "2. The Backward Hook\n",
        "\n",
        "A forward hook is excuted during the forward pass, while the backward hook is executed when `backward` function is called both of which are *functions* of `Autograd.Funciton` object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCcDR2L2E8lh",
        "colab_type": "text"
      },
      "source": [
        "A hook in PyTorch is basically a function, with a very specific signature. When we say a hook is executed, in reality, we are talkingabout this function being executed.<br>\n",
        "`grad` is basically the value contained in the `grad` attribute of the tensor **after** `backward` is called. The function is not supposed to modify it's argument. It must either return `None` or a Tensor which will be used in place of `grad` for further gradient computations.<br>\n",
        "The below example clarifies this point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt1yLHsIE-kH",
        "colab_type": "code",
        "outputId": "46825495-22c9-4cc0-b18a-53f310620006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "a = torch.ones(10)\n",
        "a.requires_grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6umlITCaE_-r",
        "colab_type": "code",
        "outputId": "39ee53ac-cf72-4d1c-fe61-2e6966584776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a.requires_grad = True\n",
        "a.requires_grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAOXhle3FMu5",
        "colab_type": "code",
        "outputId": "4db600c2-c6d9-41e2-b95a-81b7976e8600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = 2*a\n",
        "b.requires_grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBCy0MTPFNw6",
        "colab_type": "code",
        "outputId": "dbcd0199-12a9-4ea3-c8d9-207c92a2b49f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(a.is_leaf)\n",
        "print(b.is_leaf)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COtQrZf3Gb_M",
        "colab_type": "text"
      },
      "source": [
        "Since `b` is not a **leaf Variable**, its `grad` will by degault be destroyed during computation.<br>\n",
        "We can used `b.retain_grad()` to ask PyTorch to retain its `grad`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D4dnUuoGr7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b.retain_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A13yk-8XGteU",
        "colab_type": "code",
        "outputId": "c6e1459b-41ff-49df-b02c-a84687acf16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "c = b.mean()\n",
        "print(f\"requires_grad: {c.requires_grad}\")\n",
        "print(f\"is_lead: {c.is_leaf}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requires_grad: True\n",
            "is_lead: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaCBXiXdGvM7",
        "colab_type": "code",
        "outputId": "df1b209d-b5df-4a94-b7c7-d4b28dd92f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# pretend c is the loss being computed\n",
        "c.backward()\n",
        "print(a.grad, b.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
            "        0.2000]) tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
            "        0.1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnyGJjm7G9yM",
        "colab_type": "text"
      },
      "source": [
        "Now we redo the experiment but with a **hook** that multiplies `b`'s grad by 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb-2bzclHJUs",
        "colab_type": "code",
        "outputId": "6fba9014-a249-431b-85d5-da8482cbe9c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "a = torch.ones(10)\n",
        "a.requires_grad = True\n",
        "b = 2*a\n",
        "b.retain_grad()\n",
        "b.register_hook(lambda x:print(x))\n",
        "b.mean().backward() # pretend the mean of b is the loss we want to back-prop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
            "        0.1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtFRGFtlHd0t",
        "colab_type": "text"
      },
      "source": [
        "Here we can see that, the print out is exactly the same result by using **hook** on `b`, and the `lambda` function automatically take the `b.grad` as input.<br>\n",
        "This gives us a sense that hook is tracking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilIASlZvH963",
        "colab_type": "code",
        "outputId": "51f118b1-72f6-4950-f3c0-872184b45606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(a.grad, b.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
            "        0.2000]) tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
            "        0.1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJA-J_laIA_H",
        "colab_type": "text"
      },
      "source": [
        "There are several uses of functionality as above:\n",
        "1. We can print the *value* of gradient for **debugging**. We can also log them. This is especially useful with `non-leaf` variables whose gradients are freed up unless we perform `retain_grad` upon them. Doing the latter can lead to increased memory retention. Hooks provide much cleaner way to aggregate these values.\n",
        "2. We can modify gradient **during** the backward pass. This is very important. While we can still access the `grad` variable of a tensor in a network, we can only access it after the **entire backward pass** has been processed. For example, we multiplied `b`'s gradient by 2, and now the subsequent gradient calculations, like those of `a`(or any tensor that will depend upon `b` for gradient) used `2*brad(b)` instead of `grad(b)`. In contrast, had we individually updated the parameters **after** the `backward`, we'd have to multily `b.grad` as well as `a.grad`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHm9_tHdJHt-",
        "colab_type": "code",
        "outputId": "eafe0e32-3579-467d-fb5b-344a59f4a264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# to demonstrate\n",
        "a = torch.ones(10)\n",
        "a.requires_grad = True\n",
        "b = 2*a\n",
        "b.retain_grad()\n",
        "b.mean().backward()\n",
        "\n",
        "print(a.grad, b.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
            "        0.2000]) tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
            "        0.1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r6lGboZJXVn",
        "colab_type": "code",
        "outputId": "146abb5b-dd76-48e7-a2df-2491abd50910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "b.grad *= 2\n",
        "print(a.grad, b.grad) # Note that in this case, a's grad needs to be updated mannually"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
            "        0.2000]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
            "        0.2000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbaWOr3vJbWQ",
        "colab_type": "text"
      },
      "source": [
        "## Hooks for nn.Module objects\n",
        "For **backward hook**:\n",
        "`hook(module, grad_input, grad_output)`\n",
        "___\n",
        "For **forward hook**:\n",
        "`hook(module, input, output)`\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQT_tOFZJs6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3,10,2, stride=2) # (8-2+0)/2+1 = 4\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = lambda x: x.view(-1)\n",
        "        self.fc1  = nn.Linear(160,5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv(x))\n",
        "        return self.fc1(self.flatten(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I1E-eIqJvZQ",
        "colab_type": "code",
        "outputId": "2cbf707d-6957-4f9d-e74a-1c87a6941013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "Net = myNet()\n",
        "summary(Net,torch.zeros(1,3,8,8))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=======================================================\n",
            "         Kernel Shape   Output Shape Params Mult-Adds\n",
            "Layer                                                \n",
            "0_conv  [3, 10, 2, 2]  [1, 10, 4, 4]  130.0     1.92k\n",
            "1_relu              -  [1, 10, 4, 4]      -         -\n",
            "2_fc1        [160, 5]            [5]  805.0     800.0\n",
            "-------------------------------------------------------\n",
            "                      Totals\n",
            "Total params           935.0\n",
            "Trainable params       935.0\n",
            "Non-trainable params     0.0\n",
            "Mult-Adds              2.72k\n",
            "=======================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_conv</th>\n",
              "      <td>[3, 10, 2, 2]</td>\n",
              "      <td>[1, 10, 4, 4]</td>\n",
              "      <td>130.0</td>\n",
              "      <td>1920.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_relu</th>\n",
              "      <td>-</td>\n",
              "      <td>[1, 10, 4, 4]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_fc1</th>\n",
              "      <td>[160, 5]</td>\n",
              "      <td>[5]</td>\n",
              "      <td>805.0</td>\n",
              "      <td>800.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Kernel Shape   Output Shape  Params  Mult-Adds\n",
              "Layer                                                  \n",
              "0_conv  [3, 10, 2, 2]  [1, 10, 4, 4]   130.0     1920.0\n",
              "1_relu              -  [1, 10, 4, 4]     NaN        NaN\n",
              "2_fc1        [160, 5]            [5]   805.0      800.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iLsBZPUJ6hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hook_fn(m,i,o):\n",
        "    print(m)\n",
        "    print(\"---------Input Grad----------\")\n",
        "\n",
        "    for grad in i:\n",
        "        try:\n",
        "            print(grad.shape)\n",
        "        except AttributeError:\n",
        "            print(\"None found for input Gradient\")\n",
        "    \n",
        "    print(\"--------Output Grad----------\")\n",
        "    for grad in o:\n",
        "        try:\n",
        "            print(grad.shape)\n",
        "        except AttributeError:\n",
        "            print(\"None found for output Gradient\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHzpCMysq0zd",
        "colab_type": "code",
        "outputId": "377ab6c8-9591-4236-f959-3e7eaa4b4fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "Net.named_modules"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.named_modules of myNet(\n",
              "  (conv): Conv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (relu): ReLU()\n",
              "  (fc1): Linear(in_features=160, out_features=5, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m11brNsmq9J8",
        "colab_type": "code",
        "outputId": "98618e7c-a1e5-444d-cb41-3ed2e0c4b682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Net.conv.register_backward_hook(hook_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7fc70953ce80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWCYFGuZrDiN",
        "colab_type": "code",
        "outputId": "6c9ed40d-f0e7-4094-a119-cb2b442a69c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Net.fc1.register_backward_hook(hook_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7fc709536b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgqlLmXOrKhc",
        "colab_type": "code",
        "outputId": "6e4cdd1a-3958-4dca-98c1-f547d0b76ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inp = torch.rand(1,3,8,8)\n",
        "out = Net(inp)\n",
        "out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1495, -0.0683,  0.1981,  0.0851, -0.0905], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kweZoTb1rQB8",
        "colab_type": "code",
        "outputId": "14cedf15-2e41-4265-ffe9-6b86b84633c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# pretend we have the following as loss\n",
        "(1-out.mean()).backward()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=160, out_features=5, bias=True)\n",
            "---------Input Grad----------\n",
            "torch.Size([5])\n",
            "torch.Size([5])\n",
            "--------Output Grad----------\n",
            "torch.Size([5])\n",
            "\n",
            "\n",
            "Conv2d(3, 10, kernel_size=(2, 2), stride=(2, 2))\n",
            "---------Input Grad----------\n",
            "None found for input Gradient\n",
            "torch.Size([10, 3, 2, 2])\n",
            "torch.Size([10])\n",
            "--------Output Grad----------\n",
            "torch.Size([1, 10, 4, 4])\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC-_oNZPrQsX",
        "colab_type": "text"
      },
      "source": [
        "Note that, the `Linear layer` gets called first because the backward pass actually go through it first and then backprop to the `conv layer`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxa-QfPNsG__",
        "colab_type": "text"
      },
      "source": [
        "## Proper way of implementing Hooks(in **back-prop**)er way of implementing Hooks(in **back-prop**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gggioe8JvXQ2",
        "colab_type": "text"
      },
      "source": [
        "We have:\n",
        "1.  torch.autograd.Variable.register_hook (Python method, in Automatic differentiation package)\n",
        "2.  torch.nn.Module.register_backward_hook (Python method, in torch.nn)\n",
        "3.  torch.nn.Module.register_forward_hook\n",
        "\n",
        "The first `register_hook`，is for any **Variable**. It's essentially a **callback** function that is going to be executed every time when `Autograd` gradient is computed.<br>\n",
        "While `Module.register_backward_hook` & `n.Module.register_forward_hook` are for `nn.Module` object and their `hook_fn` shoud take torch:\n",
        "<br>`def hook_fn(m, i, o):` where `i` refers to input and `o` refers to output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIL-6XN9vduD",
        "colab_type": "text"
      },
      "source": [
        "### An example\n",
        "Using `named_parameters` function we can accomplish `gradient modifying/clipping`. <br>\n",
        "The following example does two things:\n",
        "1. Turn gradients of linear biases into zero while back-prop (no updates for biase)\n",
        "2. Make sure that for no gradient going to `conv layer` is less than 0 (all positive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5G2odS-vmgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3,10,2,stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = lambda x: x.view(-1)\n",
        "        self.fc1  = nn.Linear(160,5)\n",
        "    def forward(self,x):\n",
        "        x = self.relu(self.conv(x))\n",
        "        x.register_hook(lambda grad: torch.clamp(grad, min=0)) # minimun back-prop gradient of value 0\n",
        "\n",
        "        # print whether there is any negative grad\n",
        "        x.register_hook(lambda grad: print(\"Gradients less than zero:\", bool((grad<0).any())))\n",
        "        \n",
        "        return self.fc1(self.flatten(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1zPOxs9z2yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = myNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV_j9ytFz4HL",
        "colab_type": "code",
        "outputId": "17ca2a2e-a15a-4a7e-a81e-a69a18c77811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "for name, param in net.named_parameters():\n",
        "    print(name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv.weight\n",
            "conv.bias\n",
            "fc1.weight\n",
            "fc1.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSf36kx90BOa",
        "colab_type": "code",
        "outputId": "5b848d26-b650-4f71-d161-863a0c99b331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for name, param in net.named_parameters():\n",
        "    if 'fc' in name and 'bias' in name:\n",
        "        print(name, param, sep='\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fc1.bias\n",
            "Parameter containing:\n",
            "tensor([-0.0190, -0.0193, -0.0728,  0.0082,  0.0160], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMsd-dQl0PJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in net.named_parameters():\n",
        "    if 'fc' in name and 'bias' in name:\n",
        "        # assign zero to bias grad with identical dimensions\n",
        "        param.register_hook(lambda grad: torch.zeros_like(grad))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-l2x34d0kcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = net(torch.randn(1,3,8,8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bAPAHHX0pq8",
        "colab_type": "code",
        "outputId": "b26853f6-a771-40d3-9b3f-830c7a541531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(1-out).mean().backward()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradients less than zero: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quh1OR4606RE",
        "colab_type": "code",
        "outputId": "f6f4c1e2-a0ce-40ef-8a5f-8749d5ad1fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'the bias for linear layer is: {net.fc1.bias.grad}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the bias for linear layer is: tensor([0., 0., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-dt3yhb1BBk",
        "colab_type": "text"
      },
      "source": [
        "# Trick #3\n",
        "flops_counter_PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQvziiNM1IXh",
        "colab_type": "text"
      },
      "source": [
        "### Flop counter for convolutional networks in PyTorch framework\n",
        "This script is designed to compute the theoretical amount of multiply-add operations in CNNs.<br>\n",
        "It also can compute the number of parameters and print *per-layer* computational cost of a givent network.<br>\n",
        "Supported layers:\n",
        "* Conv1d/2d/3d (including grouping)\n",
        "* ConvTranspose2d (including grouping)\n",
        "* BatchNorm1d/2d/3d\n",
        "* Activations (ReLU, PReLU, ELU, ReLU6, LeakyReLU)\n",
        "* Linear\n",
        "* Upsample\n",
        "* Poolings (AvgPool1d/2d/3d, MaxPool1d/2d/3d and adaptive ones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wpSS5595cOI",
        "colab_type": "text"
      },
      "source": [
        "### Usage tips\n",
        "* This script doesn't take into account `torch.nn.functional`.* operations. For an instance, if one have a semantic segmentation model and use `torch.nn.functional.interpolate` to upscale features, these operations won't contribute to overall amount of flops. To avoid that one can use `torch.nn.Upsample` instead of `torch.nn.functional.interpolate`.\n",
        "* `ptflops` launches a given model on a random tensor and estimates amount of computations during inference. Complicated models can have several inputs, some of them could be optional. To construct non-trivial input one can use the `input_constructor` argument of the `get_model_complexity_info`. `input_constructor` is a function that takes the input spatial resolution as a tuple and returns a dict with named input arguments of the model. Next this dict would be passed to the model as keyworded arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwtE3OXY5xwz",
        "colab_type": "code",
        "outputId": "841fda5e-373c-4b5d-dce3-852bbab3ed0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "!pip install --upgrade git+https://github.com/sovrasov/flops-counter.pytorch.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/sovrasov/flops-counter.pytorch.git\n",
            "  Cloning https://github.com/sovrasov/flops-counter.pytorch.git to /tmp/pip-req-build-v7epwkpj\n",
            "  Running command git clone -q https://github.com/sovrasov/flops-counter.pytorch.git /tmp/pip-req-build-v7epwkpj\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from ptflops==0.4) (1.3.1+cu100)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch->ptflops==0.4) (1.17.3)\n",
            "Building wheels for collected packages: ptflops\n",
            "  Building wheel for ptflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptflops: filename=ptflops-0.4-cp36-none-any.whl size=7862 sha256=3c1336b31600d48dbfd47b4fb2efedee3613e1ae1326ea3c7cace7d59a885645\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f276_ger/wheels/00/ce/d1/169969eba40b2078b42c637bc9aac0f265e75a8a951b4e8570\n",
            "Successfully built ptflops\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-d5hME36DI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from Utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aah51Bxf6OXU",
        "colab_type": "code",
        "outputId": "869e0d63-f858-45db-8eb7-b144339919e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Create SSD300 with pretrained weights in the base-architecture\n",
        "n_classes = 20\n",
        "model = SSD300(n_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded base model with pre-trained weights\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wG4WiSu6SBw",
        "colab_type": "code",
        "outputId": "d9bb6578-6275-4bcb-88e8-54d8b39e9949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "with torch.cuda.device(0):\n",
        "    model = SSD300(n_classes)\n",
        "    flops, params = get_model_complexity_info(model, (3,300,300))\n",
        "    print(f'Flops: {flops}')\n",
        "    print(f'Params: {params}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded base model with pre-trained weights\n",
            "\n",
            "SSD300(\n",
            "  31.409 GMac, 100.000% MACs, \n",
            "  (base): VGGBase_BN(\n",
            "    29.983 GMac, 95.461% MACs, \n",
            "    (conv1_1): Conv2d(0.161 GMac, 0.513% MACs, 3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_1_1): BatchNorm2d(0.012 GMac, 0.037% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv1_2): Conv2d(3.324 GMac, 10.581% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_1_2): BatchNorm2d(0.012 GMac, 0.037% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (pool1): MaxPool2d(0.006 GMac, 0.018% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv2_1): Conv2d(1.662 GMac, 5.291% MACs, 64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_2_1): BatchNorm2d(0.006 GMac, 0.018% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2_2): Conv2d(3.321 GMac, 10.572% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_2_2): BatchNorm2d(0.006 GMac, 0.018% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (pool2): MaxPool2d(0.003 GMac, 0.009% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv3_1): Conv2d(1.66 GMac, 5.286% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_3_1): BatchNorm2d(0.003 GMac, 0.009% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv3_2): Conv2d(3.319 GMac, 10.568% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_3_2): BatchNorm2d(0.003 GMac, 0.009% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv3_3): Conv2d(3.319 GMac, 10.568% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_3_3): BatchNorm2d(0.003 GMac, 0.009% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (pool3): MaxPool2d(0.001 GMac, 0.005% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (conv4_1): Conv2d(1.704 GMac, 5.426% MACs, 256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_4_1): BatchNorm2d(0.001 GMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv4_2): Conv2d(3.408 GMac, 10.849% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_4_2): BatchNorm2d(0.001 GMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv4_3): Conv2d(3.408 GMac, 10.849% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_4_3): BatchNorm2d(0.001 GMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (pool4): MaxPool2d(0.001 GMac, 0.002% MACs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv5_1): Conv2d(0.852 GMac, 2.712% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_5_1): BatchNorm2d(0.0 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv5_2): Conv2d(0.852 GMac, 2.712% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_5_2): BatchNorm2d(0.0 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv5_3): Conv2d(0.852 GMac, 2.712% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn_5_3): BatchNorm2d(0.0 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (pool5): MaxPool2d(0.0 GMac, 0.001% MACs, kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    (conv6): Conv2d(1.704 GMac, 5.425% MACs, 512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
            "    (conv7): Conv2d(0.379 GMac, 1.206% MACs, 1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (aux_convs): AuxiliaryConvolutions(\n",
            "    0.231 GMac, 0.735% MACs, \n",
            "    (conv8_1): Conv2d(0.095 GMac, 0.302% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (conv8_2): Conv2d(0.118 GMac, 0.376% MACs, 256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (conv9_1): Conv2d(0.007 GMac, 0.021% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (conv9_2): Conv2d(0.007 GMac, 0.023% MACs, 128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (conv10_1): Conv2d(0.001 GMac, 0.003% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (conv10_2): Conv2d(0.003 GMac, 0.008% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (conv11_1): Conv2d(0.0 GMac, 0.001% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (conv11_2): Conv2d(0.0 GMac, 0.001% MACs, 128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "  )\n",
            "  (pred_convs): PredictionConvolutions(\n",
            "    1.195 GMac, 3.804% MACs, \n",
            "    (loc_conv4_3): Conv2d(0.106 GMac, 0.339% MACs, 512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (loc_conv7): Conv2d(0.08 GMac, 0.254% MACs, 1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (loc_conv8_2): Conv2d(0.011 GMac, 0.035% MACs, 512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (loc_conv9_2): Conv2d(0.001 GMac, 0.004% MACs, 256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (loc_conv10_2): Conv2d(0.0 GMac, 0.001% MACs, 256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (loc_conv11_2): Conv2d(0.0 GMac, 0.000% MACs, 256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cl_conv4_3): Conv2d(0.532 GMac, 1.695% MACs, 512, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cl_conv7): Conv2d(0.399 GMac, 1.271% MACs, 1024, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cl_conv8_2): Conv2d(0.055 GMac, 0.176% MACs, 512, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cl_conv9_2): Conv2d(0.007 GMac, 0.022% MACs, 256, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cl_conv10_2): Conv2d(0.002 GMac, 0.005% MACs, 256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cl_conv11_2): Conv2d(0.0 GMac, 0.001% MACs, 256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "Flops: 31.41 GMac\n",
            "Params: 26.16 M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2RMyz8L6qyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_model_complexity_info??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElRC_UyfwEOT",
        "colab_type": "text"
      },
      "source": [
        "# Trick #4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMUOgmDvR0mG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "748ef43d-e5e7-436c-cc15-6ee1159d67c6"
      },
      "source": [
        "print('hello')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeCwrRaRyUNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9okGht_py0TF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}