{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced usage for PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JuGR2A1IYmsL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sylar257/My-data-science-tool-kit/blob/master/Advanced_usage_for_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZtbRaQqSu1Z",
        "colab_type": "text"
      },
      "source": [
        "# Objective\n",
        "1. Difference between PyTorch classes like `nn.Module`, `nn.Functional`\n",
        "2. How to customize training options such as `lr` for different layers, *weight initialization*\n",
        "3. **Tensorboard** with PyTorch\n",
        "4. How to visualiza the computation graph and print it's intermediate values for **debugging**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o53MyOvZTmmg",
        "colab_type": "text"
      },
      "source": [
        "## `nn.Module` vs `nn.Functional`\n",
        "\n",
        "PyTorch layers are normally implemented with either `torch.nn.Module` objects or `torch.nn.Functional` functions.<br>\n",
        "___\n",
        "With `torch.nn.Module` we define layers in `__init__` and then invoke them in `forward` method. This is a *Object Oriented* way to do things.<br>\n",
        "On the other hand, `nn.functional` provides some **layers/activations** in form of *functions* that can be directly called on the input rather than defining as an object. E.g., in order to rescale an image tensor, we can call `torch.nn.functional.interpolate` on an image tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikMAT_niVAUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3bs2c19VV6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# simulate an input of batch_size = 1, n_channels = 3, 64x64 image\n",
        "inp = torch.randn(1,3,64,64) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quAS0LfbVcF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.nn layers\n",
        "avg_pool = nn.AvgPool2d(kernel_size=4)\n",
        "nn_out = avg_pool(inp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U3HPlGTVdcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.nn.Functional functions\n",
        "f_out  = F.avg_pool2d(inp,kernel_size=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAopzqq_WYxy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f211691-097a-4953-97da-2963ba6a794a"
      },
      "source": [
        "# check is the two are the same\n",
        "print(torch.equal(nn_out, f_out))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFd-X2qXWui8",
        "colab_type": "text"
      },
      "source": [
        "### Stateful-ness in PyTorch\n",
        "As the result of the two method are actually the same, the difference lies in their *statefulness*.<br>\n",
        "In most cases, layers can be viewed as a function in PyTorch where we feed in an input and it spits out an output.In the mean time, layers holds **weights** and **biases** that need to be stored and updated while we are training. Therefore, programmatically, layers are more than just functions but hold these *states* that will be altered during our training.<br>\n",
        "Hence, for layers that needs to hold certain **weights & biases** or any **states** we create a class for them with `torch.nn.Module`. **Batch Norm layers** and **dropout layers** behahves differently during training and inferernce which is also benificial to have them under `torch.nn.Module`.<br>\n",
        "On the other hand, `torch.nn.functional` have the layers that bear no weights and no state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuGR2A1IYmsL",
        "colab_type": "text"
      },
      "source": [
        "### nn.Parameter\n",
        "PyTorch has a `nn.Parameter` class, which will be automatically created with and `nn.Module`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XlB52UhYvnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(10,5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6n64PwqZNL8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "d8d31bef-deb9-407d-da91-206adceaee20"
      },
      "source": [
        "myNet = net()\n",
        "print(list(myNet.parameters()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.1098, -0.2625, -0.2596, -0.1384, -0.1172, -0.1551,  0.2953, -0.1297,\n",
            "          0.1755,  0.1931],\n",
            "        [ 0.1853, -0.1618, -0.3007, -0.2982,  0.1147,  0.0734,  0.1809, -0.0654,\n",
            "         -0.1599,  0.0066],\n",
            "        [-0.0894, -0.0720, -0.0499,  0.2059,  0.0093, -0.2430, -0.0026, -0.0384,\n",
            "          0.1590, -0.3022],\n",
            "        [ 0.0817,  0.0830, -0.2162,  0.2658,  0.2410,  0.0139, -0.0753, -0.2627,\n",
            "         -0.0641, -0.2089],\n",
            "        [-0.1398,  0.1820, -0.0433,  0.1104, -0.0770, -0.3056,  0.2738,  0.0522,\n",
            "         -0.0276, -0.2721]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0575,  0.2844, -0.0232,  0.2393, -0.1241], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5P4T63BZWmL",
        "colab_type": "text"
      },
      "source": [
        "Every `nn.Module` has a `.parameter()` function which will return a generator that contains its **trainable parameters**. <br>\n",
        "`nn.Parameter` is a subclass of the `Tensor` class. When we invoke `parameters()` function of a `nn.Module` object, it returns all it's members which are of `nn.Parameter` object.<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wOZIqUtave3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "1411a3ce-6fc3-4c9a-cf2f-08791d521925"
      },
      "source": [
        "# if we assign a tensor to nn.Module object, it won't show up in parameter()\n",
        "class net1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(10,5)\n",
        "        self.tensor = torch.ones(3,4) # this won't show up in parameter list\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "myNet = net1()\n",
        "print(list(myNet.parameters()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[ 0.2113, -0.2905, -0.2664, -0.1281, -0.1458, -0.0731,  0.2707,  0.1671,\n",
            "          0.1884, -0.2314],\n",
            "        [-0.1227, -0.1165,  0.2613,  0.1036, -0.0585,  0.2446,  0.3087,  0.0492,\n",
            "         -0.2182,  0.1521],\n",
            "        [-0.3053,  0.1917, -0.2265, -0.2563,  0.0678,  0.2911,  0.0997, -0.0621,\n",
            "          0.2159,  0.2285],\n",
            "        [ 0.1458,  0.0692, -0.0268,  0.2120,  0.0051, -0.2427,  0.1389, -0.2276,\n",
            "         -0.2224,  0.1383],\n",
            "        [ 0.0600, -0.2670, -0.0893, -0.1053,  0.1660, -0.2375,  0.3078, -0.2249,\n",
            "         -0.1190,  0.1370]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.0679, -0.1255,  0.2687,  0.2206,  0.2378], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1KeuZhAbavz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a63488af-aba0-4d30-9010-6721446ceff1"
      },
      "source": [
        "# if we assign a parameter to nn.Module object\n",
        "class net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(10,5)\n",
        "        self.tensor = nn.Parameter(torch.ones(3,4)) # this will show up in the parameter list later(first tensor)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "myNet = net2()\n",
        "print(list(myNet.parameters()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], requires_grad=True), Parameter containing:\n",
            "tensor([[-0.1553, -0.0954,  0.0941, -0.2285,  0.1648,  0.1840, -0.1716, -0.2685,\n",
            "          0.1683,  0.1531],\n",
            "        [-0.0428,  0.0930, -0.0076,  0.2934, -0.2999, -0.1911,  0.1036,  0.1676,\n",
            "          0.3123,  0.0017],\n",
            "        [-0.2563, -0.0477, -0.2511, -0.0582,  0.1917,  0.0220,  0.0305,  0.1232,\n",
            "         -0.0433,  0.1962],\n",
            "        [ 0.0403, -0.1390, -0.1776,  0.0375, -0.1403,  0.2554, -0.3100,  0.2137,\n",
            "          0.0873,  0.0094],\n",
            "        [-0.0458,  0.1101, -0.1114, -0.1762,  0.1499, -0.2488,  0.2555, -0.0255,\n",
            "          0.1201, -0.1327]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.1590,  0.1511, -0.3136,  0.2320,  0.2425], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtR3D9OnbqVv",
        "colab_type": "text"
      },
      "source": [
        "### nn.ModuleList and nn.ParameterList()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAgzDHgdeL8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_list = [nn.Conv2d(5,5,3),\n",
        "              nn.BatchNorm2d(5),\n",
        "              nn.Linear(5,2)]\n",
        "\n",
        "class myNet(nn.Module):\n",
        "    def __init___(self):\n",
        "        super().__init__()\n",
        "        self.layers = layer_list\n",
        "    \n",
        "    def forward(x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpUo0jNHesf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "576e7d8c-f01c-484f-9f57-5ed000267dc9"
      },
      "source": [
        "net = myNet()\n",
        "print(list(net.parameters())) # this wouldn't return of the parameters in the layers we have created"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23REK0_LeyUs",
        "colab_type": "text"
      },
      "source": [
        "We don't have any return because *Python List* does not register the parameters of Modules.<br>\n",
        "For the remedy, we need to wrap the *Python List* with `nn.ModuleList` class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDkPZHaefbpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_list = [nn.Conv2d(5,5,3),\n",
        "              nn.BatchNorm2d(5),\n",
        "              nn.Linear(5,2)]\n",
        "\n",
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layer_list)\n",
        "    def forward(x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHi06UlAffyq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65757c5e-015f-4e8a-c64a-d9e3737c6f73"
      },
      "source": [
        "net = myNet()\n",
        "\n",
        "print(list(net.parameters())) # after wrapping layer list with nn.ModuleList"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[[[ 0.0262, -0.0961,  0.0722],\n",
            "          [ 0.0927, -0.0170,  0.1368],\n",
            "          [-0.1194, -0.0112,  0.0720]],\n",
            "\n",
            "         [[-0.1425,  0.0233, -0.0219],\n",
            "          [-0.1041,  0.0985,  0.0632],\n",
            "          [-0.0270,  0.0859,  0.1119]],\n",
            "\n",
            "         [[ 0.1317,  0.0742, -0.0513],\n",
            "          [-0.0966, -0.0751, -0.0309],\n",
            "          [ 0.1291, -0.0330, -0.0163]],\n",
            "\n",
            "         [[-0.1095, -0.0301,  0.1462],\n",
            "          [-0.0381, -0.0209, -0.0848],\n",
            "          [-0.0074,  0.1206, -0.1423]],\n",
            "\n",
            "         [[ 0.0361, -0.1205, -0.0927],\n",
            "          [ 0.0280,  0.0418,  0.0856],\n",
            "          [-0.1395, -0.0219, -0.0712]]],\n",
            "\n",
            "\n",
            "        [[[-0.1226, -0.0976,  0.1239],\n",
            "          [-0.0183,  0.0369,  0.1071],\n",
            "          [ 0.1275,  0.0760,  0.0692]],\n",
            "\n",
            "         [[-0.1284, -0.0558, -0.1074],\n",
            "          [ 0.0292, -0.0241, -0.1471],\n",
            "          [ 0.0955,  0.0724, -0.0773]],\n",
            "\n",
            "         [[-0.0186,  0.0068, -0.0881],\n",
            "          [ 0.1013, -0.0403, -0.0361],\n",
            "          [ 0.1205,  0.0077, -0.0484]],\n",
            "\n",
            "         [[-0.0169, -0.0268,  0.0823],\n",
            "          [ 0.0174,  0.1210, -0.1203],\n",
            "          [ 0.0031,  0.1042,  0.0204]],\n",
            "\n",
            "         [[ 0.0507,  0.0619, -0.0133],\n",
            "          [ 0.0958,  0.0264,  0.1275],\n",
            "          [ 0.0284,  0.1446, -0.1298]]],\n",
            "\n",
            "\n",
            "        [[[-0.0722,  0.1480,  0.1450],\n",
            "          [-0.0026,  0.1407,  0.0801],\n",
            "          [-0.0058,  0.0770,  0.0010]],\n",
            "\n",
            "         [[-0.0791,  0.1056, -0.0637],\n",
            "          [-0.0752, -0.0452,  0.0896],\n",
            "          [ 0.0359,  0.0261,  0.0166]],\n",
            "\n",
            "         [[-0.0064,  0.0112, -0.0631],\n",
            "          [-0.1219, -0.0021, -0.0985],\n",
            "          [-0.0784, -0.0500,  0.0827]],\n",
            "\n",
            "         [[-0.0970, -0.0728, -0.0624],\n",
            "          [-0.0092, -0.1161,  0.0249],\n",
            "          [ 0.1340, -0.0492, -0.1229]],\n",
            "\n",
            "         [[ 0.0059, -0.0050,  0.0191],\n",
            "          [ 0.0509,  0.1160,  0.0103],\n",
            "          [-0.1443,  0.1461, -0.0428]]],\n",
            "\n",
            "\n",
            "        [[[-0.1123, -0.1415,  0.0650],\n",
            "          [-0.0115,  0.0055, -0.1139],\n",
            "          [-0.0054,  0.0959,  0.0045]],\n",
            "\n",
            "         [[-0.0652, -0.0264,  0.0721],\n",
            "          [ 0.1359, -0.0381,  0.0103],\n",
            "          [ 0.0141, -0.0290, -0.0090]],\n",
            "\n",
            "         [[ 0.0482,  0.0869, -0.0225],\n",
            "          [ 0.1084, -0.0629,  0.0517],\n",
            "          [ 0.0751,  0.1262, -0.0323]],\n",
            "\n",
            "         [[ 0.0859, -0.0608, -0.0143],\n",
            "          [ 0.0246, -0.0179,  0.0076],\n",
            "          [ 0.1360, -0.0587, -0.0019]],\n",
            "\n",
            "         [[-0.0770, -0.1317,  0.1356],\n",
            "          [-0.0272,  0.1255,  0.0230],\n",
            "          [-0.1484, -0.0358, -0.1183]]],\n",
            "\n",
            "\n",
            "        [[[-0.0206,  0.0235, -0.0745],\n",
            "          [ 0.0263,  0.1209,  0.0535],\n",
            "          [ 0.0483, -0.1443,  0.0502]],\n",
            "\n",
            "         [[ 0.0063,  0.0353, -0.1167],\n",
            "          [-0.0054,  0.0806,  0.0251],\n",
            "          [-0.0491,  0.0717, -0.1021]],\n",
            "\n",
            "         [[ 0.0501,  0.0618, -0.0456],\n",
            "          [-0.0663,  0.0506, -0.1192],\n",
            "          [ 0.0428,  0.0099,  0.0184]],\n",
            "\n",
            "         [[ 0.1108, -0.0267,  0.1167],\n",
            "          [-0.1210,  0.1183, -0.1250],\n",
            "          [ 0.0480, -0.0453,  0.0406]],\n",
            "\n",
            "         [[ 0.0945, -0.1187,  0.0061],\n",
            "          [ 0.0937,  0.1484,  0.1221],\n",
            "          [-0.0205,  0.1155,  0.1123]]]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0830, -0.1187,  0.0269, -0.0711,  0.0479], requires_grad=True), Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1.], requires_grad=True), Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\n",
            "tensor([[-0.2977, -0.4401,  0.4386, -0.0325,  0.2579],\n",
            "        [-0.4428,  0.3093, -0.3193,  0.0725, -0.0706]], requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0857, -0.3137], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYnRuzlLgBoX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f63b96e1-aa07-48d6-8b14-50d69cfca304"
      },
      "source": [
        "net.layers"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): Linear(in_features=5, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki0QJM1rgmXm",
        "colab_type": "text"
      },
      "source": [
        "## 2. Weight Initialization\n",
        "PyTorch offers different weight initialization options for us within `nn.init.`<br>\n",
        "We have `constant_`&`uniform_`; `xavier_constant_`&`xavier_uniform_`; and `Kaiming_constant_`&`Kaiming_uniform`\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e_XujCRnPTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(10,10,3)\n",
        "        self.bn   = nn.BatchNorm2d(10)\n",
        "\n",
        "    def weights_init(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std = 1.0)\n",
        "                nn.init.constant_(module.bias, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyMXqZPind4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Net = myNet()\n",
        "Net.weights_init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7puaykRn-oN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "aac7710f-ad97-4f27-f9bd-ae0057e065ed"
      },
      "source": [
        "for module in Net.modules():\n",
        "    if isinstance(module, nn.Conv2d):\n",
        "        weights = module.weight\n",
        "        weights = weights.reshape(-1).detach().cpu().numpy()\n",
        "        print(module.bias) # chech if biases are all 0\n",
        "        \n",
        "        plt.hist(weights)\n",
        "        plt.show()\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izWNswGfoluH",
        "colab_type": "text"
      },
      "source": [
        "### modules() vs children()\n",
        "The difference of the two is very slight but quite important. <br>\n",
        "As we know, a `nn.Module` object can contain other `nn.Module` objects as it's data members. <br>\n",
        "`nn.children()` will only return an *iterable* of the `nn.Module` objects which are data members. <br>\n",
        "___\n",
        "on the other hand, `nn.Modules` goes *recursively* inside each `nn.Module` object, printing each `nn.Module` object that comes along the way until there are no `nn.module` object left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcSR_Wrmsvje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convBN = nn.Sequential(nn.Conv2d(10,10,3),nn.BatchNorm2d(10))\n",
        "        self.linear = nn.Linear(10,2)\n",
        "\n",
        "    def forward(self,x):\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMiHpjkptGjc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "d53168ce-58ad-470f-f0fc-6d2e7c55e4f4"
      },
      "source": [
        "Net = myNet()\n",
        "print(\"printing children\\n----------------------------------------------\")\n",
        "print(list(Net.children()))\n",
        "print(\"\\n\\nprinting modules\\n---------------------------------------------\")\n",
        "print(list(Net.modules()))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "printing children\n",
            "----------------------------------------------\n",
            "[Sequential(\n",
            "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "), Linear(in_features=10, out_features=2, bias=True)]\n",
            "\n",
            "\n",
            "printing modules\n",
            "---------------------------------------------\n",
            "[myNet(\n",
            "  (convBN): Sequential(\n",
            "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
            "), Sequential(\n",
            "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "), Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1)), BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), Linear(in_features=10, out_features=2, bias=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qHOhGOFtfFS",
        "colab_type": "text"
      },
      "source": [
        "### Summary on printing info about model\n",
        "PyToch provides 4 convinient functions for printing out infomation our model:\n",
        "1. `named_parameters`. Return an *iterator* which gives a tuple containing **name** of the parameters (e.g., `self.conv` would have `conv.weight` and `conv.bias`)\n",
        "2. `named_modules`.  Similar to above but return `modules()` functions\n",
        "3. `named_children`. Similar to above but return `children()`\n",
        "4. `named_buffers`.  Return buffer tensors such as running mean average of a Batch Norm layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huki20dfv5Sv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "541ec5db-09ba-430f-ceee-18379acddbdf"
      },
      "source": [
        "for x in Net.named_modules():\n",
        "  print(x[0], x[1], \"\\n-------------------------------\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " myNet(\n",
            "  (convBN): Sequential(\n",
            "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
            ") \n",
            "-------------------------------\n",
            "convBN Sequential(\n",
            "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ") \n",
            "-------------------------------\n",
            "convBN.0 Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1)) \n",
            "-------------------------------\n",
            "convBN.1 BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
            "-------------------------------\n",
            "linear Linear(in_features=10, out_features=2, bias=True) \n",
            "-------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4NLXnvxwDg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "09ed410e-a003-458a-cc64-28547d66949f"
      },
      "source": [
        "for x in Net.named_children():\n",
        "  print(x[0], x[1], \"\\n-------------------------------\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convBN Sequential(\n",
            "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ") \n",
            "-------------------------------\n",
            "linear Linear(in_features=10, out_features=2, bias=True) \n",
            "-------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSIJY1RNwWuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "bc4c62aa-19cd-4838-eda5-3cc4a92682a6"
      },
      "source": [
        "for x in Net.named_children():\n",
        "  print(x[0], \"\\n-------------------------------\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convBN \n",
            "-------------------------------\n",
            "linear \n",
            "-------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfYHv3YwwhHZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "cf2a3a50-7cd4-4f2b-9b65-422484e07c1a"
      },
      "source": [
        "for x in Net.named_children():\n",
        "  print(x[1], \"\\n-------------------------------\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ") \n",
            "-------------------------------\n",
            "Linear(in_features=10, out_features=2, bias=True) \n",
            "-------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDpZgdBwwl4H",
        "colab_type": "text"
      },
      "source": [
        "## Different Learning Rates for Different Layers\n",
        "\n",
        "Now we have a solid fundation to move on to learn how to apply differnt **hyper-parametes** to different layers or layer groups.<br>\n",
        "With this, we will be able to apply different learning rate for each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHcLEK14xTtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class myNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(10,5)\n",
        "        self.fc2 = nn.Linear(5,2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.fc1(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4szPmQoxod_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Net = myNet()\n",
        "optimizer = torch.optim.SGD(Net.parameters(), lr=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7r355b8xvpZ",
        "colab_type": "text"
      },
      "source": [
        "Optionally, we can provide our optimizer with a dictionary of **learnable parameters** and their corresponding **settings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzkROdkCyW6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.SGD([\n",
        "                             {\"params\":Net.fc1.parameters(), \"lr\": 1e-3, \"momentum\":0.99},\n",
        "                             {\"params\":Net.fc2.parameters(), \"lr\": 1e-2, \"momentum\":0.9 }\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-CayC2IzJ-8",
        "colab_type": "text"
      },
      "source": [
        "## Scheduling Learning Rates\n",
        "PyTorch provides support for **learning rate scheduling** with it's `torch.optim.lr_scheduler` module. It has a variety of learning rate schedules.<br>\n",
        "Fortunately, they are all well commented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "albFuM0dzz1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,max_lr=1e-3, total_steps=1000, epochs=10,steps_per_epoch=100,pct_start=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgEcIGc71HEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}